{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_t\n",
      "    batch_size  depth  dim  dropout  emb_dropout  heads  learning_rate  mlp_dim  num_classes  patch_size  training_iteration  time_this_iter_s      loss\n",
      "85          64      7  512      0.0          0.0      7        0.00016     1024           16          16                  20        125.578363 -2.808831\n",
      "75          64      7  512      0.0          0.0      6        0.00016     1024           24          16                  20        114.581814 -2.799606\n",
      "9           64      7  512      0.0          0.0      6        0.00016     1024           32          16                  20        116.160428 -2.792183\n",
      "5           64      7  512      0.0          0.0      6        0.00016     2048           24          16                  17        146.934338 -2.643052\n",
      "83          64      7  512      0.0          0.0      6        0.00012     2048           24          16                  10        148.480175 -2.410707\n",
      "transformer_f\n",
      "    batch_size  depth   dim  dropout  emb_dropout  heads  learning_rate  mlp_dim  num_classes  patch_size  training_iteration  time_this_iter_s      loss\n",
      "54          64      7   512     0.00         0.00      6       0.000160     2048           24          16                  20         74.266267 -2.640355\n",
      "51         128      5  1024     0.05         0.00      7       0.000197     1024           24          16                  20         70.097668 -2.585782\n",
      "71         128      6   512     0.05         0.10      8       0.000189     2048           16          16                  20         75.892606 -2.583265\n",
      "72         128      6   512     0.05         0.05      8       0.000158     2048           24          16                  20         74.280954 -2.560150\n",
      "10         128      7  1024     0.10         0.00      6       0.000131     2048           32          16                  20        120.437720 -2.529079\n",
      "transformer_combined\n",
      "    batch_size  learning_rate  num_classes_f  num_classes_t  mlp_dim_f  training_iteration  time_this_iter_s      loss\n",
      "30          64       0.000160             16             16        NaN                  14        187.808537 -2.577608\n",
      "11          64       0.000160             16             16     2048.0                  14        179.116619 -2.538931\n",
      "34          64       0.000160             16             28     1024.0                   6        162.427100 -2.101107\n",
      "60          64       0.000160             12             28     1024.0                   6        162.773312 -2.090942\n",
      "65          64       0.000154             17             20        NaN                   6        189.149701 -2.059628\n",
      "transformer_mvts\n",
      "    batch_size  d_model  dim_feedforward  dropout  learning_rate  n_heads  num_layers pos_encoding  training_iteration  time_this_iter_s      loss\n",
      "47          32      128             1024     0.00         0.0001        4           6        fixed                  10         67.817078 -2.433511\n",
      "7           32      128             1024     0.00         0.0001        4           5        fixed                  10         59.700114 -2.403363\n",
      "15          32      128              512     0.00         0.0001        8           3        fixed                  10         50.817795 -2.195025\n",
      "54          32      128              512     0.05         0.0001       16           3    learnable                  10         77.781018 -1.925129\n",
      "17          64      128              256     0.10         0.0010        8           3    learnable                  10         44.307167 -0.313768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the main folder\n",
    "parent_folder_path = '/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results/'\n",
    "\n",
    "all_df = {}\n",
    "for run in ['transformer_t', 'transformer_f', 'transformer_combined', 'transformer_mvts' ]:\n",
    "\n",
    "    # Initialize an empty list to hold the data\n",
    "    data_list = []\n",
    "\n",
    "    main_folder_path = parent_folder_path + run\n",
    "    \n",
    "    # Loop through each folder in the main folder\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        folder_path = os.path.join(main_folder_path, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the JSON and CSV files in the folder\n",
    "            json_file_path = os.path.join(folder_path, 'params.json')\n",
    "            csv_file_path = os.path.join(folder_path, 'progress.csv')\n",
    "            \n",
    "            if os.path.isfile(json_file_path) and os.path.isfile(csv_file_path):\n",
    "                # Load the JSON file\n",
    "                with open(json_file_path, 'r') as json_file:\n",
    "                    json_data = json.load(json_file)\n",
    "                \n",
    "                # Load the CSV file and get the last row\n",
    "                csv_data = pd.read_csv(csv_file_path)\n",
    "                last_row = csv_data.iloc[-1]\n",
    "                \n",
    "                # Combine the JSON data and the last row of the CSV into a single dictionary\n",
    "                combined_data = {**json_data, **last_row.to_dict()}\n",
    "                \n",
    "                # Append the combined data to the list\n",
    "                data_list.append(combined_data)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = df.drop(columns=[\n",
    "        'timestamp', 'checkpoint_dir_name', 'should_checkpoint', 'done', 'trial_id', \n",
    "        'date', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', \n",
    "        'iterations_since_restore', 'max_num_epochs'\n",
    "    ])\n",
    "    \n",
    "    df = df.sort_values(by=['training_iteration', 'loss', 'time_this_iter_s'], ascending=[False, True, True])\n",
    "    duplicate_colums = [col for col in df.columns if col not in ['training_iteration', 'loss', 'time_this_iter_s']]\n",
    "    df = df.drop_duplicates(subset = duplicate_colums)\n",
    "    all_df[run] = df[duplicate_colums + ['training_iteration', 'time_this_iter_s', 'loss']]\n",
    "    \n",
    "for k,v in all_df.items():\n",
    "    print (k)\n",
    "    print (v.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{transformer_mvts}\n",
      "\\begin{tabular}{rrrllrrlrrl}\n",
      "\\toprule\n",
      "Batch size & Dimensions & MLP dim & Dropout & LR & Num heads & Depth & Positional encoding & Epochs & Time per epoch (s) & Loss \\\\\n",
      "\\midrule\n",
      "32 & 128 & 1024 & 0.00 & 1.00e-04 & 4 & 6 & fixed & 10 & 67 & -2.43 \\\\\n",
      "32 & 128 & 1024 & 0.00 & 1.00e-04 & 4 & 5 & fixed & 10 & 59 & -2.40 \\\\\n",
      "32 & 128 & 512 & 0.00 & 1.00e-04 & 8 & 3 & fixed & 10 & 50 & -2.20 \\\\\n",
      "32 & 128 & 512 & 0.05 & 1.00e-04 & 16 & 3 & learnable & 10 & 77 & -1.93 \\\\\n",
      "64 & 128 & 256 & 0.10 & 1.00e-03 & 8 & 3 & learnable & 10 & 44 & -0.31 \\\\\n",
      "64 & 128 & 1024 & 0.00 & 1.29e-04 & 4 & 6 & fixed & 7 & 58 & -2.20 \\\\\n",
      "32 & 128 & 512 & 0.00 & 1.00e-04 & 8 & 6 & fixed & 7 & 77 & -2.17 \\\\\n",
      "32 & 128 & 1024 & 0.00 & 1.00e-04 & 8 & 5 & fixed & 7 & 75 & -2.17 \\\\\n",
      "32 & 128 & 1024 & 0.00 & 1.00e-04 & 4 & 7 & fixed & 7 & 75 & -2.12 \\\\\n",
      "64 & 256 & 512 & 0.00 & 1.00e-04 & 4 & 6 & fixed & 7 & 73 & -2.08 \\\\\n",
      "32 & 128 & 1024 & 0.00 & 9.80e-04 & 4 & 6 & fixed & 1 & 67 & 0.01 \\\\\n",
      "32 & 256 & 1024 & 0.10 & 3.00e-04 & 4 & 8 & fixed & 1 & 128 & 0.02 \\\\\n",
      "64 & 128 & 1024 & 0.00 & 1.00e-03 & 4 & 5 & fixed & 1 & 52 & 0.02 \\\\\n",
      "64 & 512 & 1024 & 0.05 & 1.00e-03 & 8 & 8 & learnable & 1 & 229 & 0.02 \\\\\n",
      "32 & 256 & 1024 & 0.05 & 1.00e-03 & 8 & 5 & fixed & 1 & 105 & 0.02 \\\\\n",
      "32 & 256 & 1024 & 0.10 & 1.00e-03 & 4 & 6 & fixed & 1 & 100 & 0.03 \\\\\n",
      "32 & 256 & 1024 & 0.10 & 1.00e-03 & 4 & 4 & fixed & 1 & 74 & 0.03 \\\\\n",
      "32 & 512 & 512 & 0.10 & 1.00e-03 & 8 & 6 & fixed & 1 & 162 & 0.03 \\\\\n",
      "32 & 512 & 1024 & 0.00 & 1.00e-03 & 8 & 3 & fixed & 1 & 99 & 0.03 \\\\\n",
      "32 & 512 & 512 & 0.00 & 1.00e-03 & 4 & 6 & fixed & 1 & 137 & 0.04 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run = 'transformer_mvts'\n",
    "df = all_df[run].copy()\n",
    "\n",
    "if run == 'transformer_combined':\n",
    "    df['mlp_dim_f'] = df['mlp_dim_f'].fillna(1024)\n",
    "    df['mlp_dim_f'] = df['mlp_dim_f'].astype(int)\n",
    "    \n",
    "# Apply formatting\n",
    "try:\n",
    "    df['dropout'] = df['dropout'].apply(lambda x: f\"{x:.2f}\")\n",
    "    df['emb_dropout'] = df['emb_dropout'].apply(lambda x: f\"{x:.2f}\")\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "df['learning_rate'] = df['learning_rate'].apply(lambda x: f\"{x:.2e}\")\n",
    "df['loss'] = df['loss'].apply(lambda x: f\"{x:.2f}\")\n",
    "df['time_this_iter_s'] = df['time_this_iter_s'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# rename columns\n",
    "col_newnames = dict(\n",
    "    batch_size = \"Batch size\",\n",
    "    depth  = \"Depth\",\n",
    "    dim = \"Dimensions\",\n",
    "    dropout = \"Dropout\",\n",
    "    emb_dropout = \"EMB dropout\",\n",
    "    heads = \"Num heads\",\n",
    "    learning_rate = \"LR\",\n",
    "    mlp_dim = \"MLP dim\",\n",
    "    mlp_dim_f = \"MLP dim f\",\n",
    "    num_classes = \"Num classes\",\n",
    "    num_classes_t = \"Num classes t\",\n",
    "    num_classes_f = \"Num classes f\",\n",
    "    patch_size = \"Patch size\",\n",
    "    training_iteration  = \"Epochs\",\n",
    "    time_this_iter_s  = \"Time per epoch (s)\",\n",
    "    loss = \"Loss\",\n",
    "    d_model = \"Dimensions\",\n",
    "    dim_feedforward = \"MLP dim\",\n",
    "    n_heads = \"Num heads\",\n",
    "    num_layers = \"Depth\",\n",
    "    pos_encoding = \"Positional encoding\"\n",
    ")\n",
    "\n",
    "df = df.rename(columns=col_newnames)\n",
    "\n",
    "top_10 = df.head(10)\n",
    "bottom_10 = df.tail(10)\n",
    "\n",
    "latex_body = pd.concat([top_10, bottom_10]).to_latex(index=False, caption=run)\n",
    "\n",
    "print (latex_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    batch_size  learning_rate  num_classes_t  num_classes_f  training_iteration  time_this_iter_s      loss\n",
      "30          64       0.000160             16             16                  14        187.808537 -2.577608\n",
      "34          64       0.000160             28             16                   6        162.427100 -2.101107\n",
      "60          64       0.000160             28             12                   6        162.773312 -2.090942\n",
      "65          64       0.000154             20             17                   6        189.149701 -2.059628\n",
      "51          64       0.000160             32             32                   5        178.628841 -2.013043\n",
      "24          64       0.000105             19             17                   5        189.590113 -2.012775\n",
      "61          64       0.000135             16             15                   5        189.140957 -1.984520\n",
      "67          64       0.000104             13             16                   5        188.695028 -1.982316\n",
      "57          64       0.000160             32             20                   4        178.092067 -1.905572\n",
      "12          64       0.000120             18             13                   3        189.498463 -1.781922\n",
      "9           64       0.000187             19             21                   3        189.585180 -1.768333\n",
      "59          64       0.000158             17             15                   3        189.707342 -1.765452\n",
      "52          64       0.000150             11             24                   3        189.081856 -1.762078\n",
      "7           64       0.000160              8             32                   3        180.153333 -1.745526\n",
      "46          64       0.000075             18             19                   3        189.149197 -1.734613\n",
      "33          64       0.000160             12             24                   3        178.837139 -1.724608\n",
      "3           64       0.000160             24             20                   2        162.283022 -1.596035\n",
      "41          64       0.000160             12             28                   2        178.215241 -1.592923\n",
      "53          64       0.000160             20             20                   2        162.128272 -1.573402\n",
      "35          64       0.000104             19             21                   2        188.144151 -1.566886\n",
      "13          64       0.000085             14             16                   2        188.900612 -1.544896\n",
      "26          32       0.000130             21             14                   2        203.553252 -1.536334\n",
      "17          64       0.000053             14             23                   2        188.835155 -1.532820\n",
      "21          32       0.000100             27             23                   2        201.982415 -1.532483\n",
      "37          64       0.000069             21             19                   2        189.022359 -1.521571\n",
      "28          64       0.000165             12             17                   2        188.962738 -1.518631\n",
      "56          64       0.000063             26             18                   2        188.946722 -1.514621\n",
      "32          64       0.000257              8             26                   2        190.029507 -1.511935\n",
      "10          64       0.000219             25             24                   2        189.449309 -1.508220\n",
      "23          64       0.000241             17              9                   2        188.445855 -1.503526\n",
      "14          64       0.000052             20             22                   2        189.736540 -1.493646\n",
      "1           64       0.000051             18             18                   2        188.927471 -1.481872\n",
      "64          32       0.000042             10             30                   2        203.858512 -1.480381\n",
      "58          64       0.000160              8             16                   1        179.455450 -1.202983\n",
      "19          64       0.000160             16              8                   1        163.578696 -1.190724\n",
      "27          64       0.000160             16             12                   1        179.457832 -1.173666\n",
      "50          64       0.000160             24              8                   1        163.296509 -1.162566\n",
      "49          64       0.000160             20              8                   1        179.571642 -1.162178\n",
      "48          32       0.000215             15             13                   1        202.669049 -1.151093\n",
      "29          64       0.000283             15             10                   1        190.278740 -1.104101\n",
      "43          32       0.000026             28             26                   1        203.111639 -1.100607\n",
      "0           64       0.000328              9             20                   1        190.423260 -1.078608\n",
      "18          32       0.000020             23             30                   1        203.724337 -1.037351\n",
      "6           32       0.000197             29              9                   1        204.339688 -1.036712\n",
      "63          64       0.000036             23             13                   1        189.577688 -1.036467\n",
      "55          32       0.000025             17             12                   1        203.760169 -1.032956\n",
      "4           64       0.000362             28             14                   1        189.977684 -1.012035\n",
      "5           32       0.000020             11             16                   1        204.147262 -0.969274\n",
      "36          64       0.000030             16             14                   1        189.633880 -0.967521\n",
      "66          64       0.000371             30             27                   1        190.333837 -0.961450\n",
      "39          32       0.000016             20              8                   1        204.662344 -0.935298\n",
      "15          32       0.000322             23             30                   1        202.902733 -0.929450\n",
      "22          64       0.000027             10             30                   1        189.656775 -0.904868\n",
      "69          64       0.000485              8             29                   1        189.390802 -0.869056\n",
      "68          32       0.000434             22             11                   1        204.708555 -0.709743\n",
      "54          64       0.000747             22             20                   1        189.345454 -0.702308\n",
      "40          32       0.000459             25             32                   1        204.044979 -0.699317\n",
      "45          64       0.000554             23             10                   1        188.839302 -0.683619\n",
      "31          64       0.000632             24             26                   1        190.171052 -0.555489\n",
      "62          32       0.000716             15             11                   1        203.842376 -0.505268\n",
      "8           64       0.000011             23             12                   1        190.088364 -0.367740\n",
      "25          64       0.000012             32             21                   1        191.772094 -0.359467\n",
      "44          32       0.000891             14             29                   1        202.934617 -0.113813\n",
      "47          64       0.000975             13             12                   1        190.485662  0.004854\n"
     ]
    }
   ],
   "source": [
    "print (df_filtered.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peregrine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
