{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import numpy as np\n",
    "import swyft.lightning as sl\n",
    "from toolz.dicttoolz import valmap\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Unet implementation below\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        mid_channels=None,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                mid_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(\n",
    "                mid_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_sampling=2):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(down_sampling), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)       \n",
    "\n",
    "class Up(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, scale_factor, kernel_size=3, stride=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.up = nn.ConvTranspose1d(\n",
    "        #     in_channels, in_channels // 2, kernel_size=kernel_size, stride=stride\n",
    "        # )\n",
    "        \n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=scale_factor),\n",
    "            nn.ConvTranspose1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.att = AttentionGate(out_channels, out_channels // 2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        \n",
    "        # x1 = gate\n",
    "        # x2 = skip-connection\n",
    "        \n",
    "        #print ('Up x1 (g)', g.shape)\n",
    "        #print ('Up x2 (x)', x.shape)\n",
    "        \n",
    "        # diff_signal_length = x2.size()[2] - x1.size()[2]\n",
    "        # x1 = F.pad(\n",
    "        #     x1, [diff_signal_length // 2, diff_signal_length - diff_signal_length // 2]\n",
    "        # )\n",
    "        \n",
    "        x1 = self.up(g); #print ('Up x1', x1.shape)\n",
    "        s = self.att(x1, x); #print ('Up s', s.shape)\n",
    "        \n",
    "        x = torch.cat([s, x1], dim=1)\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.Wx = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, 1, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        \n",
    "        # g = gate\n",
    "        # x = skip-connection\n",
    "        \n",
    "        #print ('Att (g)', g.shape)\n",
    "        #print ('Att (x)', x.shape)\n",
    "        \n",
    "        Wg = self.Wg(g); #print ('Att (Wg)', Wg.shape)\n",
    "        \n",
    "        Wx = self.Wx(x); #print ('Att (Wx)', Wx.shape)\n",
    "        out = self.relu(Wg + Wx); #print ('Att (out)', out.shape)\n",
    "        out = self.psi(out); #print ('Att (out)', out.shape)\n",
    "        \n",
    "        # print (out)\n",
    "        \n",
    "        out = out * x; #print ('Att (out)', out.shape)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in_channels,\n",
    "        n_out_channels,\n",
    "        sizes=(16, 32, 64, 128, 256),\n",
    "        down_sampling=(2, 2, 2, 2),\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        self.inc = DoubleConv(n_in_channels, sizes[0])\n",
    "        self.down1 = Down(sizes[0], sizes[1], down_sampling[0])\n",
    "        self.down2 = Down(sizes[1], sizes[2], down_sampling[1])\n",
    "        self.down3 = Down(sizes[2], sizes[3], down_sampling[2])\n",
    "        self.down4 = Down(sizes[3], sizes[4], down_sampling[3])\n",
    "        self.up1 = Up(sizes[4], sizes[3], down_sampling[3])\n",
    "        self.up2 = Up(sizes[3], sizes[2], down_sampling[2])\n",
    "        self.up3 = Up(sizes[2], sizes[1], down_sampling[1])\n",
    "        self.up4 = Up(sizes[1], sizes[0], down_sampling[0])\n",
    "        self.outc = OutConv(sizes[0], n_out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (x.shape)\n",
    "        x1 = self.inc(x); #print ('x1', x1.shape)\n",
    "        x2 = self.down1(x1); #print ('x2', x2.shape)\n",
    "        x3 = self.down2(x2); #print ('x3', x3.shape)\n",
    "        x4 = self.down3(x3); #print ('x4', x4.shape)\n",
    "        x5 = self.down4(x4); #print ('x5', x5.shape)\n",
    "        x = self.up1(x5, x4); #print (x.shape)\n",
    "        x = self.up2(x, x3); #print (x.shape)\n",
    "        x = self.up3(x, x2); #print (x.shape)\n",
    "        x = self.up4(x, x1); #print (x.shape)\n",
    "        f = self.outc(x); #print (f.shape)\n",
    "        return f\n",
    "\n",
    "\n",
    "class LinearCompression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearCompression, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_t = torch.rand(5,3,8192).to(device)\n",
    "data_f = torch.rand(5,6,4097).to(device)\n",
    "\n",
    "unet_t = Unet(\n",
    "    n_in_channels=3, \n",
    "    n_out_channels=1,\n",
    "    sizes=(16, 32, 64, 128, 256),\n",
    "    down_sampling=(8, 8, 8, 8),\n",
    ").to(device)\n",
    "\n",
    "unet_f = Unet(\n",
    "    n_in_channels=6, \n",
    "    n_out_channels=1,\n",
    "    sizes=(16, 32, 64, 128, 256),\n",
    "    down_sampling=(2, 2, 2, 2),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0206, -0.3780, -0.0874,  ...,  0.0109,  0.2588,  0.3941]],\n",
       "\n",
       "        [[-0.0722,  0.0090,  0.0559,  ..., -0.0300, -0.0280,  0.1172]],\n",
       "\n",
       "        [[ 0.0403,  0.6867,  0.4020,  ...,  0.1316,  0.3188,  0.1926]],\n",
       "\n",
       "        [[-0.0856, -0.8712, -0.8845,  ..., -0.0046,  0.0463,  0.4451]],\n",
       "\n",
       "        [[ 0.2795,  0.1790,  0.3238,  ..., -0.0909,  0.1677,  0.3383]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_f(data_f[:,:,:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
