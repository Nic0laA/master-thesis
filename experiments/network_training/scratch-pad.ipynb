{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/scur2012/Thesis/master-thesis/experiments/network_training')\n",
    "\n",
    "import zarr\n",
    "import swyft.lightning as sl\n",
    "\n",
    "zarr_store_dirs = '/scratch-shared/scur2012/peregrine_data/tmnre_experiments'\n",
    "name_of_run = 'peregrine_copy_highSNR_v3'\n",
    "\n",
    "rnd_id = 2\n",
    "simulation_store_path = f\"{zarr_store_dirs}/{name_of_run}/simulations/round_{rnd_id+1}\"\n",
    "zarr_store = sl.ZarrStore(f\"{simulation_store_path}\")\n",
    "\n",
    "simulation_results = zarr.convenience.open(simulation_store_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gw_parameters\n",
    "importlib.reload(gw_parameters)\n",
    "\n",
    "conf = gw_parameters.default_conf\n",
    "bounds = gw_parameters.limits\n",
    "\n",
    "# Settings for trainer and network\n",
    "\n",
    "trainer_settings = dict(\n",
    "    min_epochs = 30,\n",
    "    max_epochs = 200,\n",
    "    early_stopping = 7,\n",
    "    num_workers = 8,\n",
    "    training_batch_size = 256,\n",
    "    validation_batch_size = 256,\n",
    "    train_split = 0.9,\n",
    "    val_split = 0.1\n",
    ")\n",
    "\n",
    "network_settings = dict(\n",
    "    # Peregrine\n",
    "    shuffling = True,\n",
    "    priors = dict(\n",
    "        int_priors = conf['priors']['int_priors'],\n",
    "        ext_priors = conf['priors']['ext_priors'],\n",
    "    ),\n",
    "    marginals = ((0, 1),),\n",
    "    one_d_only = True,\n",
    "    ifo_list = conf[\"waveform_params\"][\"ifo_list\"],\n",
    "    learning_rate = 5e-4,\n",
    "    training_batch_size = trainer_settings['training_batch_size'],\n",
    "    save_path = '/home/scur2012/Thesis/master-thesis/experiments/network_training/roc_curve'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/Thesis/master-thesis/.venv/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2012/Thesis/master-thesis/.venv/lib/python ...\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Initialise dataloaders\n",
    "\n",
    "train_data = zarr_store.get_dataloader(\n",
    "    num_workers=trainer_settings['num_workers'],\n",
    "    batch_size=trainer_settings['training_batch_size'],\n",
    "    idx_range=[0, int(trainer_settings['train_split'] * len(zarr_store.data.z_int))],\n",
    "    on_after_load_sample=False,\n",
    ")\n",
    "\n",
    "val_data = zarr_store.get_dataloader(\n",
    "    num_workers=trainer_settings['num_workers'],\n",
    "    batch_size=trainer_settings['validation_batch_size'],\n",
    "    idx_range=[\n",
    "        int(trainer_settings['train_split'] * len(zarr_store.data.z_int)),\n",
    "        len(zarr_store.data.z_int) - 1,\n",
    "    ],\n",
    "    on_after_load_sample=None,\n",
    ")\n",
    "\n",
    "# Set up the pytorch trainer settings\n",
    "\n",
    "tmp_dir = f\"tmp_dir\"\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=trainer_settings[\"early_stopping\"],\n",
    "    verbose=False,\n",
    "    mode=\"min\",\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=f\"{tmp_dir}\",\n",
    "    filename=\"{epoch}_{val_loss:.2f}_{train_loss:.2f}\" + f\"_round_{rnd_id+1}\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Make directory for logger\n",
    "os.makedirs(f'{tmp_dir}/logs', exist_ok=True)\n",
    "logger_tbl = pl_loggers.TensorBoardLogger(\n",
    "    save_dir=f\"{tmp_dir}\",\n",
    "    name=f\"logs\",\n",
    "    version=None,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "\n",
    "swyft_trainer = sl.SwyftTrainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    min_epochs=trainer_settings[\"min_epochs\"],\n",
    "    max_epochs=trainer_settings[\"max_epochs\"],\n",
    "    logger=logger_tbl,\n",
    "    callbacks=[lr_monitor, early_stopping_callback, checkpoint_callback],\n",
    "    enable_progress_bar = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'peregrine_network' from '/gpfs/home3/scur2012/Thesis/master-thesis/experiments/network_training/peregrine_network.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import peregrine_network\n",
    "from peregrine_network import InferenceNetwork\n",
    "importlib.reload(peregrine_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/Thesis/master-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Load network model\n",
    "ckpt = f\"/scratch-shared/scur2012/peregrine_data/tmnre_experiments/peregrine_copy_highSNR_v3/training/round_1/epoch=59_val_loss=-4.26_train_loss=-4.24_round_1.ckpt\"\n",
    "# ckpt = f\"/scratch-shared/scur2012/peregrine_data/tmnre_experiments/peregrine_copy_highSNR_v3/training/round_7/epoch=28_val_loss=-5.16_train_loss=-5.23_round_7.ckpt\"\n",
    "# checkpoint = torch.load(ckpt)\n",
    "\n",
    "network = InferenceNetwork.load_from_checkpoint(ckpt, **network_settings)\n",
    "# network = InferenceNetwork(**network_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/Thesis/master-thesis/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory tmp_dir exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/scur2012/Thesis/master-thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name         | Type                   | Params\n",
      "--------------------------------------------------------\n",
      "0 | unet_t       | Unet                   | 722 K \n",
      "1 | unet_f       | Unet                   | 722 K \n",
      "2 | flatten      | Flatten                | 0     \n",
      "3 | linear_t     | LinearCompression      | 0     \n",
      "4 | linear_f     | LinearCompression      | 0     \n",
      "5 | logratios_1d | LogRatioEstimator_1dim | 290 K \n",
      "--------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.942     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a521276c412547f4910c364fba78a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8483ba0e702c407bb066b20019443e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badab5884d1e4ce6a482ecf9fb1f2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit data to model\n",
    "\n",
    "swyft_trainer.fit(network, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# model = InferenceNetwork.load_from_checkpoint(ckpt, **network_settings)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=network_settings['learning_rate'])\n",
    "# \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# \n",
    "# # Start training the model\n",
    "# \n",
    "# # enable gradient calculation\n",
    "# torch.set_grad_enabled(True)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_data):\n",
    "\n",
    "    break\n",
    "\n",
    "    loss = model.training_step(batch, batch_idx)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()   \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print (batch_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.dicttoolz import valmap\n",
    "\n",
    "A = batch\n",
    "B = valmap(lambda z: torch.roll(z, 1, dims=0), A)\n",
    "\n",
    "x = A\n",
    "z = {}\n",
    "for key in B:\n",
    "    z[key] = torch.cat([A[key], B[key]])\n",
    "\n",
    "num_pos = len(list(x.values())[0])  # Number of positive examples\n",
    "num_neg = len(list(z.values())[0]) - num_pos  # Number of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['z_total'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logratios = model._get_logratios( model(x,z) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros_like(logratios)\n",
    "y[:num_pos, ...] = 1\n",
    "pos_weight = torch.ones_like(logratios[0]) * num_neg / num_pos\n",
    "probabilities = torch.nn.functional.softmax(logratios, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "\n",
    "plt.figure()\n",
    "for i, param_name in enumerate(bounds.keys()):\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y.numpy()[:,i], probabilities.detach().numpy()[:,i])\n",
    "    roc_auc = auc(fpr, tpr)  # Calculate area under the curve\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, lw=1, label=f'{param_name} (area = {roc_auc :0.2f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(bbox_to_anchor=(1.7, 1), loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
