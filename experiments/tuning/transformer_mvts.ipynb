{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "\n",
    "import zarr\n",
    "import swyft.lightning as sl\n",
    "\n",
    "data_directory = '/scratch-shared/scur2012/peregrine_data/bhardwaj2023'\n",
    "\n",
    "run_name = 'lowSNR'\n",
    "rnd_id = 1\n",
    "\n",
    "data_dir = f'/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}'\n",
    "# simulation_results = zarr.convenience.open(simulation_store_path)\n",
    "\n",
    "data_dir = '/scratch-shared/scur2012/peregrine_data/tmnre_experiments/peregrine_copy_highSNR_v3/simulations/round_1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "\n",
    "def load_data(data_dir, batch_size=64, train_test_split=0.9):\n",
    "    \n",
    "    zarr_store = sl.ZarrStore(f\"{data_dir}\")\n",
    "    \n",
    "    train_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[0, int(train_test_split * len(zarr_store.data.z_int))],\n",
    "        on_after_load_sample=False\n",
    "    )\n",
    "\n",
    "    val_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[\n",
    "            int(train_test_split * len(zarr_store.data.z_int)),\n",
    "            len(zarr_store.data.z_int) - 1,\n",
    "        ],\n",
    "        on_after_load_sample=None\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model \n",
    "# https://github.com/gzerveas/mvts_transformer/blob/master/src/models/ts_transformer.py\n",
    "\n",
    "from typing import Optional, Any\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "# From https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "class FixedPositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(max_len, 1, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(pos_encoding))\n",
    "\n",
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None, is_causal: bool = False) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src\n",
    "\n",
    "class TSTransformerEncoderClassiregressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplest classifier/regressor. Can be either regressor or classifier because the output does not include\n",
    "    softmax. Concatenates final layer embeddings and uses 0s to ignore padding embeddings in final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        feat_dim, \n",
    "        max_len, \n",
    "        d_model, \n",
    "        n_heads, \n",
    "        num_layers, \n",
    "        dim_feedforward, \n",
    "        num_classes,\n",
    "        patch_size=16,\n",
    "        dropout=0.1, \n",
    "        pos_encoding='fixed', \n",
    "        activation='gelu', \n",
    "        norm='BatchNorm', \n",
    "        freeze=False\n",
    "        ):\n",
    "        super(TSTransformerEncoderClassiregressor, self).__init__()\n",
    "\n",
    "        assert (max_len % patch_size) == 0\n",
    "\n",
    "        num_patches = max_len // patch_size\n",
    "        patch_dim = max_len * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "        )\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.output_layer = self.build_output_module(d_model, max_len, num_classes)\n",
    "\n",
    "    def build_output_module(self, d_model, max_len, num_classes):\n",
    "        output_layer = nn.Linear(d_model * max_len, num_classes)\n",
    "        # no softmax (or log softmax), because CrossEntropyLoss does this internally. If probabilities are needed,\n",
    "        # add F.log_softmax and use NLLoss\n",
    "        return output_layer\n",
    "\n",
    "    def forward(self, X, padding_masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        \n",
    "        print (X.shape)\n",
    "        \n",
    "        inp = self.to_patch_embedding(X)\n",
    "        \n",
    "        print (inp.shape)\n",
    "        \n",
    "        inp = inp.permute(2, 0, 1)\n",
    "        \n",
    "        print (inp.shape)\n",
    "        \n",
    "        inp = self.project_inp(inp) * math.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        \n",
    "        print (inp.shape)\n",
    "        \n",
    "        inp = self.pos_enc(inp)  # add positional encoding\n",
    "                \n",
    "        print (inp.shape)\n",
    "                \n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "        output = self.transformer_encoder(inp, src_key_padding_mask=~padding_masks)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity        \n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "        \n",
    "        # Output\n",
    "        output = output * padding_masks.unsqueeze(-1)  # zero-out padding embeddings\n",
    "        output = output.reshape(output.shape[0], -1)  # (batch_size, seq_length * d_model)\n",
    "        output = self.output_layer(output)  # (batch_size, num_classes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inference network\n",
    "import numpy as np\n",
    "import swyft.lightning as sl\n",
    "from toolz.dicttoolz import valmap\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTInferenceNetwork(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size, \n",
    "        learning_rate,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "        pos_encoding,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_t = TSTransformerEncoderClassiregressor(\n",
    "            feat_dim = 3,\n",
    "            max_len = 8192,\n",
    "            num_classes = 16,\n",
    "            d_model = d_model,\n",
    "            n_heads = n_heads,\n",
    "            num_layers = num_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout,\n",
    "            pos_encoding = pos_encoding,\n",
    "            activation = 'gelu',\n",
    "            norm = 'BatchNorm', \n",
    "            freeze = False,\n",
    "        )\n",
    "        \n",
    "        self.ViT_f = TSTransformerEncoderClassiregressor(\n",
    "            feat_dim = 6,\n",
    "            max_len = 4096,\n",
    "            num_classes = 16,\n",
    "            d_model = d_model,\n",
    "            n_heads = n_heads,\n",
    "            num_layers = num_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout,\n",
    "            pos_encoding = pos_encoding,\n",
    "            activation = 'gelu',\n",
    "            norm = 'BatchNorm', \n",
    "            freeze = False,\n",
    "        )\n",
    "        \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=32, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        features_t = self.ViT_t(d_t, torch.ones(d_t.shape[0], d_t.shape[2]).to(torch.bool).to(device))\n",
    "        features_f = self.ViT_f(d_f_w[:,:,:-1], torch.ones(d_f_w[:,:,:-1].shape[0], d_f_w[:,:,:-1].shape[2]).to(torch.bool).to(device))\n",
    "        \n",
    "        features = torch.cat([features_t, features_f], dim=1)\n",
    "        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint, CheckpointConfig, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import CombinedStopper,MaximumIterationStopper,TrialPlateauStopper\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_transformer_mvts(config, data_dir=None):\n",
    "    \n",
    "    print (config)\n",
    "    \n",
    "    net = ViTInferenceNetwork(        \n",
    "        batch_size = config['batch_size'], \n",
    "        learning_rate = config['learning_rate'],\n",
    "        d_model = config['d_model'],\n",
    "        n_heads = config['n_heads'], \n",
    "        num_layers = config['num_layers'],\n",
    "        dim_feedforward = config['dim_feedforward'],\n",
    "        dropout = config['dropout'],\n",
    "        pos_encoding = config['pos_encoding'],\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        \n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n",
    "            start_epoch = checkpoint_dict[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_dict[\"model_state\"])\n",
    "        \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainloader, valloader = load_data(data_dir, batch_size=config['batch_size'], train_test_split=0.8)\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"max_num_epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "                \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = net.training_step(batch, 0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            #if i % 20 == 19:  # print every 20 mini-batches\n",
    "            #    print(\n",
    "            #        \"[%d, %5d] loss: %.6f\"\n",
    "            #        % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "            #    )\n",
    "            #    running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        \n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "                loss = net.validation_step(batch, 0)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "\n",
    "        metrics = {\"loss\": val_loss / val_steps}\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch, \"model_state\": net.state_dict()},\n",
    "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "            )\n",
    "            train.report(metrics=metrics, checkpoint=Checkpoint.from_directory(tempdir))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10):\n",
    "    \n",
    "    data_dir = os.path.abspath(f\"/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}\")\n",
    "        \n",
    "    config = dict(\n",
    "        batch_size = tune.choice([1]),\n",
    "        learning_rate = tune.choice([1e-3, 3e-4, 1e-4]),\n",
    "        d_model = tune.choice([128,256,512]),\n",
    "        n_heads = tune.choice([4,8,16]),\n",
    "        num_layers = tune.choice([3,4,5,6,7,8]),\n",
    "        dim_feedforward = tune.choice([256,512,1024]),\n",
    "        dropout = tune.choice([0, 0.05, 0.1]),\n",
    "        pos_encoding = tune.choice(['fixed','learnable']),\n",
    "        max_num_epochs = max_num_epochs,\n",
    "    )\n",
    "    \n",
    "    first_guess = [{\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"d_model\": 128,\n",
    "        \"n_heads\": 8,\n",
    "        \"num_layers\": 3,\n",
    "        \"dim_feedforward\": 256,\n",
    "        \"dropout\": 0.1,\n",
    "        \"pos_encoding\": 'learnable',\n",
    "    }]\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=2500,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "        time_attr=\"time_total_s\"\n",
    "    )\n",
    "        \n",
    "    tune_config = TuneConfig(\n",
    "        max_concurrent_trials=1,\n",
    "        num_samples=num_samples,\n",
    "        search_alg=HyperOptSearch(\n",
    "            points_to_evaluate=first_guess, \n",
    "            metric='loss', \n",
    "            mode='min', \n",
    "            n_initial_points=10),\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    \n",
    "    stopper = CombinedStopper(\n",
    "        MaximumIterationStopper(max_iter=max_num_epochs),\n",
    "        TrialPlateauStopper(metric=\"loss\"),\n",
    "    )\n",
    "    \n",
    "    run_config = RunConfig(\n",
    "        name=\"transformer_mvts\",\n",
    "        storage_path='/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results',\n",
    "        checkpoint_config=CheckpointConfig(checkpoint_score_attribute='loss', checkpoint_score_order='min',num_to_keep=1),\n",
    "        log_to_file=True,\n",
    "        stop=stopper,\n",
    "    )\n",
    "    \n",
    "    # Create Tuner\n",
    "    trainable_with_cpu_gpu = tune.with_resources(partial(train_transformer_mvts, data_dir=data_dir), {\"cpu\": 18, \"gpu\": 1})\n",
    "    tuner = Tuner(\n",
    "        trainable_with_cpu_gpu,\n",
    "        # Add some parameters to tune\n",
    "        param_space=config,\n",
    "        # Specify tuning behavior\n",
    "        tune_config=tune_config,\n",
    "        # Specify run behavior\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    \n",
    "    # Run tuning job\n",
    "    results = tuner.fit()\n",
    "    print(results.get_best_result(metric=\"loss\", mode=\"min\").config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main(num_samples=100, max_num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 8192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[131072], expected input with shape [*, 131072], but got input of size[5, 512, 48]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m trainloader, valloader \u001b[38;5;241m=\u001b[39m load_data(data_dir, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, train_test_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m (i)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 227\u001b[0m, in \u001b[0;36mTSTransformerEncoderClassiregressor.forward\u001b[0;34m(self, X, padding_masks)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 227\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_patch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m (inp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    231\u001b[0m inp \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[131072], expected input with shape [*, 131072], but got input of size[5, 512, 48]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.exit()\n",
    "\n",
    "net = TSTransformerEncoderClassiregressor(\n",
    "    feat_dim = 3,\n",
    "    max_len = 8192,\n",
    "    num_classes = 16,\n",
    "    d_model = 128,\n",
    "    n_heads = 16, \n",
    "    num_layers = 3,\n",
    "    dim_feedforward = 256,\n",
    "    dropout = 0.1,\n",
    "    pos_encoding = 'learnable',\n",
    "    activation = 'gelu',\n",
    "    norm = 'BatchNorm', \n",
    "    freeze = False,\n",
    "    )\n",
    "\n",
    "trainloader, valloader = load_data(data_dir, batch_size=5, train_test_split=0.8)\n",
    "            \n",
    "for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    out = net(data['d_t'], torch.ones(data['d_t'].shape[0], data['d_t'].shape[2]).to(torch.bool) )\n",
    "    \n",
    "    print (i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
