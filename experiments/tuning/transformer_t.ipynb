{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "\n",
    "import zarr\n",
    "import swyft.lightning as sl\n",
    "\n",
    "data_directory = '/scratch-shared/scur2012/peregrine_data/bhardwaj2023'\n",
    "\n",
    "run_name = 'lowSNR'\n",
    "rnd_id = 1\n",
    "\n",
    "data_dir = f'/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}'\n",
    "# simulation_results = zarr.convenience.open(simulation_store_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "\n",
    "def load_data(data_dir, batch_size=64, train_test_split=0.9):\n",
    "    \n",
    "    zarr_store = sl.ZarrStore(f\"{data_dir}\")\n",
    "    \n",
    "    train_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[0, int(train_test_split * len(zarr_store.data.z_int))],\n",
    "        on_after_load_sample=False\n",
    "    )\n",
    "\n",
    "    val_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[\n",
    "            int(train_test_split * len(zarr_store.data.z_int)),\n",
    "            len(zarr_store.data.z_int) - 1,\n",
    "        ],\n",
    "        on_after_load_sample=None\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model \n",
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_1d.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (seq_len % patch_size) == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, series):\n",
    "        \n",
    "        x = self.to_patch_embedding(series)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
    "        x, ps = pack([cls_tokens, x], 'b * d')\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls_tokens, _ = unpack(x, ps, 'b * d')\n",
    "\n",
    "        return self.mlp_head(cls_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inference network\n",
    "import numpy as np\n",
    "import swyft.lightning as sl\n",
    "from toolz.dicttoolz import valmap\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTInferenceNetwork(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_t = ViT(\n",
    "            seq_len = 8192,\n",
    "            channels = 3,\n",
    "            patch_size = 16,\n",
    "            num_classes = 16,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.ViT_f = ViT(\n",
    "            seq_len = 4096,\n",
    "            channels = 6,\n",
    "            patch_size = 16,\n",
    "            num_classes = 16,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=32, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features_t = self.ViT_t(d_t)\n",
    "        features_f = self.ViT_f(d_f_w[:,:,:-1])\n",
    "        \n",
    "        features = torch.cat([features_t, features_f], dim=1)\n",
    "        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    "    \n",
    "    \n",
    "class ViTInferenceNetwork_t(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size, \n",
    "        learning_rate, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        dim, \n",
    "        depth, \n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dropout,\n",
    "        emb_dropout\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_t = ViT(\n",
    "            seq_len = 8192,\n",
    "            channels = 3,\n",
    "            patch_size = patch_size,\n",
    "            num_classes = num_classes,\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            mlp_dim = mlp_dim,\n",
    "            dropout = dropout,\n",
    "            emb_dropout = emb_dropout,\n",
    "        )\n",
    "               \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=num_classes, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features = self.ViT_t(d_t)        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    "    \n",
    "class ViTInferenceNetwork_f(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size, \n",
    "        learning_rate, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        dim, \n",
    "        depth, \n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dropout,\n",
    "        emb_dropout\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_f = ViT(\n",
    "            seq_len = 4096,\n",
    "            channels = 6,\n",
    "            patch_size = patch_size,\n",
    "            num_classes = num_classes,\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            mlp_dim = mlp_dim,\n",
    "            dropout = dropout,\n",
    "            emb_dropout = emb_dropout,\n",
    "        )\n",
    "               \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=num_classes, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features = self.ViT_f(d_f_w[:,:,:-1])        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint, CheckpointConfig, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import CombinedStopper,MaximumIterationStopper,TrialPlateauStopper\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_transformer_t(config, data_dir=None):\n",
    "    \n",
    "    print (config)\n",
    "    \n",
    "    net = ViTInferenceNetwork_t(        \n",
    "        batch_size = config['batch_size'], \n",
    "        learning_rate = config['learning_rate'], \n",
    "        patch_size = config['patch_size'], \n",
    "        num_classes = config['num_classes'], \n",
    "        dim = config['dim'], \n",
    "        depth = config['depth'], \n",
    "        heads = config['heads'],\n",
    "        mlp_dim = config['mlp_dim'],\n",
    "        dropout = config['dropout'],\n",
    "        emb_dropout = config['emb_dropout']\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        \n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n",
    "            start_epoch = checkpoint_dict[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_dict[\"model_state\"])\n",
    "        \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainloader, valloader = load_data(data_dir, batch_size=config['batch_size'], train_test_split=0.8)\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"max_num_epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "                \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = net.training_step(batch, 0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            #if i % 20 == 19:  # print every 20 mini-batches\n",
    "            #    print(\n",
    "            #        \"[%d, %5d] loss: %.6f\"\n",
    "            #        % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "            #    )\n",
    "            #    running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        \n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "                loss = net.validation_step(batch, 0)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "\n",
    "        metrics = {\"loss\": val_loss / val_steps}\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch, \"model_state\": net.state_dict()},\n",
    "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "            )\n",
    "            train.report(metrics=metrics, checkpoint=Checkpoint.from_directory(tempdir))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10):\n",
    "    \n",
    "    data_dir = os.path.abspath(f\"/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}\")\n",
    "        \n",
    "    config = {\n",
    "        \"batch_size\": tune.choice([64]),\n",
    "        \"learning_rate\": tune.choice([1.6e-4]),\n",
    "        \"patch_size\": tune.choice([16]),\n",
    "        \"num_classes\": tune.choice([16, 24, 32]),\n",
    "        \"dim\": tune.choice([512, 1024]),\n",
    "        \"depth\": tune.choice([6,7]),\n",
    "        \"heads\": tune.choice([6, 7, 8]),\n",
    "        \"mlp_dim\": tune.choice([1024, 2048]),\n",
    "        \"dropout\": tune.choice([0]),                          \n",
    "        \"emb_dropout\": tune.choice([0, 0.1]),\n",
    "        \"max_num_epochs\": max_num_epochs,\n",
    "    }\n",
    "    \n",
    "    first_guess = [{\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 1.6e-4,\n",
    "        \"patch_size\": 16,\n",
    "        \"num_classes\": 24,\n",
    "        \"dim\": 512,\n",
    "        \"depth\": 7,\n",
    "        \"heads\": 6,\n",
    "        \"mlp_dim\": 2048,\n",
    "        \"dropout\": 0,\n",
    "        \"emb_dropout\": 0,\n",
    "    }]\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=2500,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "        time_attr=\"time_total_s\"\n",
    "    )\n",
    "        \n",
    "    tune_config = TuneConfig(\n",
    "        max_concurrent_trials=1,\n",
    "        num_samples=num_samples,\n",
    "        search_alg=HyperOptSearch(points_to_evaluate=first_guess, metric='loss', mode='min'),\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    \n",
    "    stopper = CombinedStopper(\n",
    "        MaximumIterationStopper(max_iter=max_num_epochs),\n",
    "        TrialPlateauStopper(metric=\"loss\"),\n",
    "    )\n",
    "    \n",
    "    run_config = RunConfig(\n",
    "        name=\"transformer_t\",\n",
    "        storage_path='/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results',\n",
    "        checkpoint_config=CheckpointConfig(checkpoint_score_attribute='loss', checkpoint_score_order='min'),\n",
    "        log_to_file=True,\n",
    "        stop=stopper,\n",
    "    )\n",
    "    \n",
    "    # Create Tuner\n",
    "    trainable_with_cpu_gpu = tune.with_resources(partial(train_transformer_t, data_dir=data_dir), {\"cpu\": 18, \"gpu\": 1})\n",
    "    tuner = Tuner(\n",
    "        trainable_with_cpu_gpu,\n",
    "        # Add some parameters to tune\n",
    "        param_space=config,\n",
    "        # Specify tuning behavior\n",
    "        tune_config=tune_config,\n",
    "        # Specify run behavior\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    \n",
    "    # Run tuning job\n",
    "    results = tuner.fit()\n",
    "    print(results.get_best_result(metric=\"loss\", mode=\"min\").config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:58:53,061\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-04-28 14:58:53,067\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-04-28 14:58:53,068\tINFO tune.py:1004 -- Wrote the latest version of all result files and experiment state to '/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results/transformer_t' in 0.0058s.\n",
      "2024-04-28 14:58:54,211\tINFO tune.py:1036 -- Total run time: 11.72 seconds (10.51 seconds for the tuning loop).\n",
      "2024-04-28 14:58:54,211\tWARNING tune.py:1051 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results/transformer_t\", trainable=...)\n",
      "2024-04-28 14:58:54,213\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_transformer_t_03676235: FileNotFoundError('Could not fetch metrics for train_transformer_t_03676235: both result.json and progress.csv were not found at /home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results/transformer_t/train_transformer_t_03676235_1_batch_size=64,depth=6,dim=512,dropout=0,emb_dropout=0.1000,heads=8,learning_rate=0.0002,mlp_dim=102_2024-04-28_14-58-42')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "main(num_samples=100, max_num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
