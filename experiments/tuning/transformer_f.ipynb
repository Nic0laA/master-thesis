{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "\n",
    "import zarr\n",
    "import swyft.lightning as sl\n",
    "\n",
    "data_directory = '/scratch-shared/scur2012/peregrine_data/bhardwaj2023'\n",
    "\n",
    "run_name = 'lowSNR'\n",
    "rnd_id = 1\n",
    "\n",
    "data_dir = f'/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}'\n",
    "# simulation_results = zarr.convenience.open(simulation_store_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "\n",
    "def load_data(data_dir, batch_size=64, train_test_split=0.9):\n",
    "    \n",
    "    zarr_store = sl.ZarrStore(f\"{data_dir}\")\n",
    "    \n",
    "    train_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[0, int(train_test_split * len(zarr_store.data.z_int))],\n",
    "        on_after_load_sample=False\n",
    "    )\n",
    "\n",
    "    val_data = zarr_store.get_dataloader(\n",
    "        num_workers=8,\n",
    "        batch_size=batch_size,\n",
    "        idx_range=[\n",
    "            int(train_test_split * len(zarr_store.data.z_int)),\n",
    "            len(zarr_store.data.z_int) - 1,\n",
    "        ],\n",
    "        on_after_load_sample=None\n",
    "    )\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model \n",
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_1d.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (seq_len % patch_size) == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, series):\n",
    "        \n",
    "        x = self.to_patch_embedding(series)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
    "        x, ps = pack([cls_tokens, x], 'b * d')\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls_tokens, _ = unpack(x, ps, 'b * d')\n",
    "\n",
    "        return self.mlp_head(cls_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Inference network\n",
    "import numpy as np\n",
    "import swyft.lightning as sl\n",
    "from toolz.dicttoolz import valmap\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTInferenceNetwork(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_t = ViT(\n",
    "            seq_len = 8192,\n",
    "            channels = 3,\n",
    "            patch_size = 16,\n",
    "            num_classes = 16,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.ViT_f = ViT(\n",
    "            seq_len = 4096,\n",
    "            channels = 6,\n",
    "            patch_size = 16,\n",
    "            num_classes = 16,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=32, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features_t = self.ViT_t(d_t)\n",
    "        features_f = self.ViT_f(d_f_w[:,:,:-1])\n",
    "        \n",
    "        features = torch.cat([features_t, features_f], dim=1)\n",
    "        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    "    \n",
    "    \n",
    "class ViTInferenceNetwork_t(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size, \n",
    "        learning_rate, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        dim, \n",
    "        depth, \n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dropout,\n",
    "        emb_dropout\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_t = ViT(\n",
    "            seq_len = 8192,\n",
    "            channels = 3,\n",
    "            patch_size = patch_size,\n",
    "            num_classes = num_classes,\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            mlp_dim = mlp_dim,\n",
    "            dropout = dropout,\n",
    "            emb_dropout = emb_dropout,\n",
    "        )\n",
    "               \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=num_classes, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features = self.ViT_t(d_t)        \n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    "    \n",
    "class ViTInferenceNetwork_f(sl.SwyftModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size, \n",
    "        learning_rate, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        dim, \n",
    "        depth, \n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dropout,\n",
    "        emb_dropout\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "\n",
    "        self.ViT_f = ViT(\n",
    "            seq_len = 4096,\n",
    "            channels = 6,\n",
    "            patch_size = patch_size,\n",
    "            num_classes = num_classes,\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            mlp_dim = mlp_dim,\n",
    "            dropout = dropout,\n",
    "            emb_dropout = emb_dropout,\n",
    "        )\n",
    "               \n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=num_classes, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=learning_rate)\n",
    "\n",
    "    def forward(self, A, B):        \n",
    "                \n",
    "        if self.include_noise:\n",
    "                   \n",
    "            if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "                noise_shuffling = torch.randperm(self.batch_size)\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "            else:\n",
    "                d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "                d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        \n",
    "        else:\n",
    "            d_t = A[\"d_t\"]\n",
    "            d_f_w = A[\"d_f_w\"]\n",
    "        \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        features = self.ViT_f(d_f_w[:,:,:-1])\n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint, CheckpointConfig, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.stopper import CombinedStopper,MaximumIterationStopper,TrialPlateauStopper\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_transformer_f(config, data_dir=None):\n",
    "    \n",
    "    print (config)\n",
    "    \n",
    "    net = ViTInferenceNetwork_f(        \n",
    "        batch_size = config['batch_size'], \n",
    "        learning_rate = config['learning_rate'], \n",
    "        patch_size = config['patch_size'], \n",
    "        num_classes = config['num_classes'], \n",
    "        dim = config['dim'], \n",
    "        depth = config['depth'], \n",
    "        heads = config['heads'],\n",
    "        mlp_dim = config['mlp_dim'],\n",
    "        dropout = config['dropout'],\n",
    "        emb_dropout = config['emb_dropout']\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        \n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n",
    "            start_epoch = checkpoint_dict[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_dict[\"model_state\"])\n",
    "        \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainloader, valloader = load_data(data_dir, batch_size=config['batch_size'], train_test_split=0.8)\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"max_num_epochs\"]):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "                \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = net.training_step(batch, 0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            #if i % 20 == 19:  # print every 20 mini-batches\n",
    "            #    print(\n",
    "            #        \"[%d, %5d] loss: %.6f\"\n",
    "            #        % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "            #    )\n",
    "            #    running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        \n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                batch = {key:data[key].to(device) for key in data}\n",
    "\n",
    "                loss = net.validation_step(batch, 0)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "\n",
    "        metrics = {\"loss\": val_loss / val_steps}\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch, \"model_state\": net.state_dict()},\n",
    "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "            )\n",
    "            train.report(metrics=metrics, checkpoint=Checkpoint.from_directory(tempdir))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10):\n",
    "    \n",
    "    data_dir = os.path.abspath(f\"/scratch-shared/scur2012/peregrine_data/bhardwaj2023/simulations_{run_name}_R{rnd_id}\")\n",
    "        \n",
    "    config = {\n",
    "        \"batch_size\": tune.sample_from(lambda spec: spec.config.patch_size * 8),\n",
    "        \"learning_rate\": tune.loguniform(3e-5, 2e-4),\n",
    "        \"patch_size\": tune.choice([4,8,16]),\n",
    "        \"num_classes\": tune.choice([16, 24, 32]),\n",
    "        \"dim\": tune.choice([256, 512, 1024]),\n",
    "        \"depth\": tune.randint(4,10),\n",
    "        \"heads\": tune.randint(4,10),\n",
    "        \"mlp_dim\": tune.choice([1024, 2048]),\n",
    "        \"dropout\": tune.choice([0,0.05,0.1]),\n",
    "        \"emb_dropout\": tune.choice([0,0.05,0.1]),\n",
    "        \"max_num_epochs\": max_num_epochs,\n",
    "    }\n",
    "    \n",
    "    first_guess = [{\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1.6e-4,\n",
    "        \"patch_size\": 4,\n",
    "        \"num_classes\": 24,\n",
    "        \"dim\": 512,\n",
    "        \"depth\": 7,\n",
    "        \"heads\": 6,\n",
    "        \"mlp_dim\": 2048,\n",
    "        \"dropout\": 0,\n",
    "        \"emb_dropout\": 0,\n",
    "    }]\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=2500,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "        time_attr=\"time_total_s\"\n",
    "    )\n",
    "        \n",
    "    tune_config = TuneConfig(\n",
    "        max_concurrent_trials=1,\n",
    "        num_samples=num_samples,\n",
    "        search_alg=HyperOptSearch(points_to_evaluate=first_guess, metric='loss', mode='min'),\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    \n",
    "    stopper = CombinedStopper(\n",
    "        MaximumIterationStopper(max_iter=max_num_epochs),\n",
    "        TrialPlateauStopper(metric=\"loss\"),\n",
    "    )\n",
    "    \n",
    "    run_config = RunConfig(\n",
    "        name=\"transformer_f\",\n",
    "        storage_path='/home/scur2012/Thesis/master-thesis/experiments/tuning/ray_results',\n",
    "        checkpoint_config=CheckpointConfig(checkpoint_score_attribute='loss', checkpoint_score_order='min'),\n",
    "        log_to_file=True,\n",
    "        stop=stopper,\n",
    "    )\n",
    "    \n",
    "    # Create Tuner\n",
    "    trainable_with_cpu_gpu = tune.with_resources(partial(train_transformer_f, data_dir=data_dir), {\"cpu\": 18, \"gpu\": 1})\n",
    "    tuner = Tuner(\n",
    "        trainable_with_cpu_gpu,\n",
    "        # Add some parameters to tune\n",
    "        param_space=config,\n",
    "        # Specify tuning behavior\n",
    "        tune_config=tune_config,\n",
    "        # Specify run behavior\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    \n",
    "    # Run tuning job\n",
    "    results = tuner.fit()\n",
    "    print(results.get_best_result(metric=\"loss\", mode=\"min\").config)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 13:11:29,166\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs)\u001b[0m\n\u001b[1;32m     66\u001b[0m tuner \u001b[38;5;241m=\u001b[39m Tuner(\n\u001b[1;32m     67\u001b[0m     trainable_with_cpu_gpu,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Add some parameters to tune\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Run tuning job\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39mget_best_result(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/tune/tuner.py:377\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes hyperparameter tuning job as configured and returns result.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03mFailure handling:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    RayTaskError: If user-provided trainable raises an exception\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m     (\n\u001b[1;32m    380\u001b[0m         progress_reporter,\n\u001b[1;32m    381\u001b[0m         string_queue,\n\u001b[1;32m    382\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_remote_tuner_for_jupyter_progress_reporting()\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:476\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_restored:\n\u001b[0;32m--> 476\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:595\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[1;32m    583\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    594\u001b[0m }\n\u001b[0;32m--> 595\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_remote_string_queue()\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/tune/tune.py:527\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     error_message_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune.run(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_space_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestore_entrypoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune.run(..., resume=True)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    525\u001b[0m     }\n\u001b[0;32m--> 527\u001b[0m \u001b[43m_ray_auto_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_message_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentrypoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remote \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     _remote \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mray\u001b[38;5;241m.\u001b[39mis_connected()\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/tune/tune.py:252\u001b[0m, in \u001b[0;36m_ray_auto_init\u001b[0;34m(entrypoint)\u001b[0m\n\u001b[1;32m    250\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTUNE_DISABLE_AUTO_INIT=1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[0;32m--> 252\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Ray automatically. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor cluster usage or custom Ray initialization, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall `ray.init(...)` before `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentrypoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/_private/worker.py:1751\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(info_str)\n\u001b[0;32m-> 1751\u001b[0m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_global_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_driver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_object_store_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_driver_object_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entrypoint_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mand\u001b[39;00m job_config\u001b[38;5;241m.\u001b[39mcode_search_path:\n\u001b[1;32m   1764\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mset_load_code_from_local(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/ray/_private/worker.py:2362\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint, worker_launch_time_ms, worker_launched_time_ms)\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2360\u001b[0m     logs_dir \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_logs_dir_path()\n\u001b[0;32m-> 2362\u001b[0m worker\u001b[38;5;241m.\u001b[39mcore_worker \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raylet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreWorker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplasma_store_socket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraylet_socket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_ip_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_manager_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraylet_ip_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLOCAL_MODE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stdout_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stderr_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized_job_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_agent_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruntime_env_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartup_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSCRIPT_MODE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_launch_time_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_launched_time_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;66;03m# Notify raylet that the core worker is ready.\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mnotify_raylet()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(num_samples=50, max_num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
