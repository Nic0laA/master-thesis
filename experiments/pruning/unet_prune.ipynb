{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_pruning as tp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import numpy as np\n",
    "import swyft.lightning as sl\n",
    "from toolz.dicttoolz import valmap\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class InferenceNetwork(sl.SwyftModule):\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.one_d_only = True       \n",
    "        self.batch_size = conf[\"batch_size\"]\n",
    "        self.noise_shuffling = True\n",
    "        self.num_params = 15\n",
    "        self.marginals = (0,1),\n",
    "        self.include_noise = True\n",
    "        \n",
    "        self.unet_t = Unet(\n",
    "            n_in_channels=3,\n",
    "            n_out_channels=1,\n",
    "            sizes=(16, 32, 64, 128, 256),\n",
    "            down_sampling=(8, 8, 8, 8),\n",
    "        )\n",
    "        self.unet_f = Unet(\n",
    "            n_in_channels=6,\n",
    "            n_out_channels=1,\n",
    "            sizes=(16, 32, 64, 128, 256),\n",
    "            down_sampling=(4, 2, 2, 2),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.linear_t = LinearCompression()\n",
    "        self.linear_f = LinearCompression()\n",
    "\n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=32, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=conf[\"learning_rate\"])\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        \n",
    "        if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "            noise_shuffling = torch.randperm(self.batch_size)\n",
    "            d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "            d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "        else:\n",
    "            d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "            d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        d_t = self.unet_t(d_t)\n",
    "        d_f_w = self.unet_f(d_f_w)\n",
    "\n",
    "        features_t = self.linear_t(self.flatten(d_t))\n",
    "        features_f = self.linear_f(self.flatten(d_f_w))\n",
    "        features = torch.cat([features_t, features_f], dim=1)\n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        return logratios_1d\n",
    "               \n",
    "# 1D Unet implementation below\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        mid_channels=None,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                mid_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(\n",
    "                mid_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_sampling=2):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(down_sampling), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose1d(\n",
    "            in_channels, in_channels // 2, kernel_size=kernel_size, stride=stride\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diff_signal_length = x2.size()[2] - x1.size()[2]\n",
    "\n",
    "        x1 = F.pad(\n",
    "            x1, [diff_signal_length // 2, diff_signal_length - diff_signal_length // 2]\n",
    "        )\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in_channels,\n",
    "        n_out_channels,\n",
    "        sizes=(16, 32, 64, 128, 256),\n",
    "        down_sampling=(2, 2, 2, 2),\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        self.inc = DoubleConv(n_in_channels, sizes[0])\n",
    "        self.down1 = Down(sizes[0], sizes[1], down_sampling[0])\n",
    "        self.down2 = Down(sizes[1], sizes[2], down_sampling[1])\n",
    "        self.down3 = Down(sizes[2], sizes[3], down_sampling[2])\n",
    "        self.down4 = Down(sizes[3], sizes[4], down_sampling[3])\n",
    "        self.up1 = Up(sizes[4], sizes[3])\n",
    "        self.up2 = Up(sizes[3], sizes[2])\n",
    "        self.up3 = Up(sizes[2], sizes[1])\n",
    "        self.up4 = Up(sizes[1], sizes[0])\n",
    "        self.outc = OutConv(sizes[0], n_out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        f = self.outc(x)\n",
    "        return f\n",
    "\n",
    "\n",
    "class LinearCompression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearCompression, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet(\n",
    "    n_in_channels=3,\n",
    "    n_out_channels=1,\n",
    "    sizes=(16, 32, 64, 128, 256),\n",
    "    down_sampling=(8, 8, 8, 8),\n",
    ")\n",
    "example_inputs = torch.randn(5, 3, 8192)\n",
    "\n",
    "device = 'cpu'\n",
    "checkpoint_path = \"models/unet_peregrine/epoch=41-step=305000_val_loss=-5.51_train_loss=-5.75.ckpt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model_params = model.state_dict()\n",
    "pretrained_params = {k[7:]: v for k, v in checkpoint['state_dict'].items() if 'unet_t' in k}\n",
    "model_params.update(pretrained_params)\n",
    "model.load_state_dict(pretrained_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/scratch-shared/scur2012/training_data/default_limits_2e6/training_data'\n",
    "zarr_store = sl.ZarrStore(f\"{data_dir}\")\n",
    "\n",
    "train_data = zarr_store.get_dataloader(\n",
    "    num_workers=8,\n",
    "    batch_size=256,\n",
    "    idx_range=[0, 1000],\n",
    "    on_after_load_sample=False,\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(train_data):\n",
    "    #loss = model.training_step(batch, batch_idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 8192])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['d_t'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imp, tp\u001b[38;5;241m.\u001b[39mimportance\u001b[38;5;241m.\u001b[39mGroupTaylorImportance):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Taylor expansion requires gradients for importance estimation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(example_inputs) \n\u001b[0;32m---> 55\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# before pruner.step()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m pruner\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m macs, nparams \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcount_ops_and_params(model, example_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/autograd/__init__.py:259\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    250\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    251\u001b[0m     (inputs,)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    258\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.conda/envs/peregrine/lib/python3.11/site-packages/torch/autograd/__init__.py:132\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    136\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "for tf in ['t','f']:\n",
    "    for ratio in [0, 0.05, 0.10, 0.2, 0.3, 0.4, 0.5]:\n",
    "\n",
    "        if tf == 't':\n",
    "            model = Unet(\n",
    "                n_in_channels=3,\n",
    "                n_out_channels=1,\n",
    "                sizes=(16, 32, 64, 128, 256),\n",
    "                down_sampling=(8, 8, 8, 8),\n",
    "            )\n",
    "            #example_inputs = torch.randn(5, 3, 8192)\n",
    "            example_inputs = batch['d_t']\n",
    "        else:\n",
    "            model = Unet(\n",
    "                n_in_channels=6,\n",
    "                n_out_channels=1,\n",
    "                sizes=(16, 32, 64, 128, 256),\n",
    "                down_sampling=(2, 2, 2, 2),\n",
    "            )\n",
    "            example_inputs = torch.randn(5, 6, 4097)\n",
    "            example_inputs = batch['d_f_w']\n",
    "\n",
    "        checkpoint_path = \"models/unet_peregrine/epoch=41-step=305000_val_loss=-5.51_train_loss=-5.75.ckpt\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "        model_params = model.state_dict()\n",
    "        pretrained_params = {k[7:]: v for k, v in checkpoint['state_dict'].items() if f'unet_{tf}' in k}\n",
    "        model_params.update(pretrained_params)\n",
    "        model.load_state_dict(pretrained_params)\n",
    "\n",
    "\n",
    "        # 1. Importance criterion\n",
    "        imp = tp.importance.GroupNormImportance(p=2) #GroupTaylorImportance() # or GroupNormImportance(p=2), GroupHessianImportance(), etc.\n",
    "\n",
    "        # 2. Initialize a pruner with the model and the importance criterion\n",
    "        ignored_layers = []\n",
    "        # for m in model.modules():\n",
    "        #     if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "        #         ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "        pruner = tp.pruner.MetaPruner( # We can always choose MetaPruner if sparse training is not required.\n",
    "            model,\n",
    "            example_inputs,\n",
    "            importance=imp,\n",
    "            pruning_ratio=ratio, # remove 20% channels\n",
    "            # pruning_ratio_dict = {model.conv1: 0.2, model.layer2: 0.8}, # customized pruning ratios for layers or blocks\n",
    "            ignored_layers=ignored_layers,\n",
    "        )\n",
    "\n",
    "        # 3. Prune & finetune the model\n",
    "        base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "        if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
    "            # Taylor expansion requires gradients for importance estimation\n",
    "            loss = model(example_inputs) \n",
    "            loss.backward() # before pruner.step()\n",
    "\n",
    "        pruner.step()\n",
    "        macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "        # finetune the pruned model here\n",
    "        # finetune(model)\n",
    "        # ...\n",
    "\n",
    "        # Save models\n",
    "        model.zero_grad() # clear gradients\n",
    "        torch.save(model, f'unet_{tf}_pruned_{ratio}_taylor.pth')\n",
    "        \n",
    "        print (ratio, tf, base_macs, base_nparams, macs, nparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 8192])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('/home/scur2012/Thesis/master-thesis/experiments/pruning/unet_t_pruned_0.5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class InferenceNetworkPruned(InferenceNetwork):\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__(**conf)\n",
    "        self.unet_t = torch.load('/home/scur2012/Thesis/master-thesis/experiments/pruning/unet_t_pruned_0.5.pth')\n",
    "        self.unet_f = torch.load('/home/scur2012/Thesis/master-thesis/experiments/pruning/unet_f_pruned_0.5.pth')\n",
    "\n",
    "network = InferenceNetworkPruned(batch_size=256, learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InferenceNetworkPruned(\n",
       "  (unet_t): Unet(\n",
       "    (inc): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv1d(3, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (down1): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down2): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down3): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down4): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): Up(\n",
       "      (up): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): Up(\n",
       "      (up): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): Up(\n",
       "      (up): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): Up(\n",
       "      (up): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (outc): OutConv(\n",
       "      (conv): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (unet_f): Unet(\n",
       "    (inc): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv1d(6, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (down1): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down2): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down3): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down4): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): Up(\n",
       "      (up): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): Up(\n",
       "      (up): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): Up(\n",
       "      (up): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): Up(\n",
       "      (up): ConvTranspose1d(16, 8, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (outc): OutConv(\n",
       "      (conv): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_t): LinearCompression(\n",
       "    (sequential): Sequential(\n",
       "      (0): LazyLinear(in_features=0, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): LazyLinear(in_features=0, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_f): LinearCompression(\n",
       "    (sequential): Sequential(\n",
       "      (0): LazyLinear(in_features=0, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): LazyLinear(in_features=0, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (logratios_1d): LogRatioEstimator_1dim(\n",
       "    (ptrans): ParameterTransform(\n",
       "      (online_z_score): OnlineStandardizingLayer()\n",
       "    )\n",
       "    (classifier): MarginalClassifier(\n",
       "      (net): ResidualNetWithChannel(\n",
       "        (initial_layer): LinearWithChannel()\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x ResidualBlockWithChannel(\n",
       "            (batch_norm_layers): ModuleList(\n",
       "              (0-1): 2 x BatchNorm1dWithChannel(\n",
       "                960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (linear_layers): ModuleList(\n",
       "              (0-1): 2 x LinearWithChannel()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layer): LinearWithChannel()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InferenceNetwork(\n",
       "  (unet_t): Unet(\n",
       "    (inc): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv1d(3, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (down1): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down2): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down3): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down4): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): Up(\n",
       "      (up): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): Up(\n",
       "      (up): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): Up(\n",
       "      (up): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): Up(\n",
       "      (up): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (outc): OutConv(\n",
       "      (conv): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (unet_f): Unet(\n",
       "    (inc): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv1d(6, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (down1): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down2): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down3): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down4): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up1): Up(\n",
       "      (up): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up2): Up(\n",
       "      (up): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up3): Up(\n",
       "      (up): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up4): Up(\n",
       "      (up): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,))\n",
       "      (conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "          (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (outc): OutConv(\n",
       "      (conv): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_t): LinearCompression(\n",
       "    (sequential): Sequential(\n",
       "      (0): LazyLinear(in_features=0, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): LazyLinear(in_features=0, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_f): LinearCompression(\n",
       "    (sequential): Sequential(\n",
       "      (0): LazyLinear(in_features=0, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): LazyLinear(in_features=0, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (logratios_1d): LogRatioEstimator_1dim(\n",
       "    (ptrans): ParameterTransform(\n",
       "      (online_z_score): OnlineStandardizingLayer()\n",
       "    )\n",
       "    (classifier): MarginalClassifier(\n",
       "      (net): ResidualNetWithChannel(\n",
       "        (initial_layer): LinearWithChannel()\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x ResidualBlockWithChannel(\n",
       "            (batch_norm_layers): ModuleList(\n",
       "              (0-1): 2 x BatchNorm1dWithChannel(\n",
       "                960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (linear_layers): ModuleList(\n",
       "              (0-1): 2 x LinearWithChannel()\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layer): LinearWithChannel()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferenceNetwork(batch_size=256, learning_rate=5e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peregrine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
