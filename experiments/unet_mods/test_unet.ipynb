{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gw_parameters' from '/gpfs/home3/scur2012/Thesis/master-thesis/experiments/tmnre/gw_parameters.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "os.chdir('/home/scur2012/Thesis/master-thesis/experiments/tmnre')\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import swyft.lightning as sl\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "\n",
    "import importlib\n",
    "import gw_parameters\n",
    "importlib.reload(gw_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Unet implementation below\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        mid_channels=None,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                mid_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(\n",
    "                mid_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down_sampling=2):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(down_sampling), DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose1d(\n",
    "            in_channels, in_channels // 2, kernel_size=kernel_size, stride=stride\n",
    "        )\n",
    "        # self.up = nn.Sequential(\n",
    "        #     nn.Upsample(scale_factor=scale_factor, mode='linear', align_corners=True),\n",
    "        #     nn.ConvTranspose1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        #     nn.BatchNorm1d(out_channels),\n",
    "        #     nn.ReLU(inplace=True)\n",
    "        # )\n",
    "        self.att = AttentionGate(out_channels, out_channels // 2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        diff_signal_length = x2.size()[2] - x1.size()[2]\n",
    "        x1 = F.pad(\n",
    "            x1, [diff_signal_length // 2, diff_signal_length - diff_signal_length // 2]\n",
    "        )\n",
    "        \n",
    "        s = self.att(x1, x2)\n",
    "        x = torch.cat([s, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.Wx = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, 1, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        \n",
    "        # g = gate\n",
    "        # x = skip-connection\n",
    "\n",
    "        Wg = self.Wg(g)\n",
    "        Wx = self.Wx(x)\n",
    "        out = self.relu(Wg + Wx)\n",
    "        out = self.psi(out)\n",
    "        out = out * x\n",
    "        return out\n",
    "    \n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in_channels,\n",
    "        n_out_channels,\n",
    "        sizes=(16, 32, 64, 128, 256),\n",
    "        down_sampling=(2, 2, 2, 2),\n",
    "    ):\n",
    "        super(Unet, self).__init__()\n",
    "        self.inc = DoubleConv(n_in_channels, sizes[0])\n",
    "        self.down1 = Down(sizes[0], sizes[1], down_sampling[0])\n",
    "        self.down2 = Down(sizes[1], sizes[2], down_sampling[1])\n",
    "        self.down3 = Down(sizes[2], sizes[3], down_sampling[2])\n",
    "        self.down4 = Down(sizes[3], sizes[4], down_sampling[3])\n",
    "        self.up1 = Up(sizes[4], sizes[3], down_sampling[3])\n",
    "        self.up2 = Up(sizes[3], sizes[2], down_sampling[2])\n",
    "        self.up3 = Up(sizes[2], sizes[1], down_sampling[1])\n",
    "        self.up4 = Up(sizes[1], sizes[0], down_sampling[0])\n",
    "        self.outc = OutConv(sizes[0], n_out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        f = self.outc(x)\n",
    "        return f\n",
    "\n",
    "\n",
    "class LinearCompression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearCompression, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network architecture\n",
    "\n",
    "class InferenceNetwork(sl.SwyftModule):\n",
    "    def __init__(self, **conf):\n",
    "        super().__init__()\n",
    "        self.one_d_only = conf[\"one_d_only\"]\n",
    "        self.batch_size = conf[\"training_batch_size\"]\n",
    "        self.noise_shuffling = conf[\"shuffling\"]\n",
    "        self.num_params = len(conf[\"priors\"][\"int_priors\"].keys()) + len(\n",
    "            conf[\"priors\"][\"ext_priors\"].keys()\n",
    "        )\n",
    "        self.marginals = conf[\"marginals\"]\n",
    "        \n",
    "        self.netw_t = Unet(\n",
    "            n_in_channels=len(conf[\"ifo_list\"]),\n",
    "            n_out_channels=1,\n",
    "            sizes=(16, 32, 64, 128, 256),\n",
    "            down_sampling=(8, 8, 8, 8),\n",
    "        )\n",
    "        \n",
    "        self.netw_f = Unet(\n",
    "            n_in_channels=2 * len(conf[\"ifo_list\"]),\n",
    "            n_out_channels=1,\n",
    "            sizes=(16, 32, 64, 128, 256),\n",
    "            down_sampling=(2, 2, 2, 2),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.linear_t = LinearCompression()\n",
    "        self.linear_f = LinearCompression()\n",
    "\n",
    "        self.logratios_1d = sl.LogRatioEstimator_1dim(\n",
    "            num_features=32, num_params=int(self.num_params), varnames=\"z_total\"\n",
    "        )\n",
    "            \n",
    "        self.optimizer_init = sl.AdamOptimizerInit(lr=conf[\"learning_rate\"])\n",
    "        \n",
    "    def forward(self, A, B):        \n",
    "                   \n",
    "        if self.noise_shuffling and A[\"d_t\"].size(0) != 1:\n",
    "            noise_shuffling = torch.randperm(self.batch_size)\n",
    "            d_t = A[\"d_t\"] + A[\"n_t\"][noise_shuffling]\n",
    "            d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"][noise_shuffling]\n",
    "        else:\n",
    "            d_t = A[\"d_t\"] + A[\"n_t\"]\n",
    "            d_f_w = A[\"d_f_w\"] + A[\"n_f_w\"]\n",
    "       \n",
    "        z_total = B[\"z_total\"]\n",
    "\n",
    "        d_t = self.netw_t(d_t)\n",
    "        d_f_w = self.netw_f(d_f_w[:,:,:-1])\n",
    "        flatten_t = self.flatten(d_t)\n",
    "        flatten_f = self.flatten(d_f_w)\n",
    "        features_t = self.linear_t(flatten_t)\n",
    "        features_f = self.linear_f(flatten_f)\n",
    "        \n",
    "        features = torch.cat([features_t, features_f], dim=1)\n",
    "        logratios_1d = self.logratios_1d(features, z_total)\n",
    "        \n",
    "        return logratios_1d\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for trainer and network\n",
    "\n",
    "conf = gw_parameters.default_conf\n",
    "bounds = gw_parameters.limits\n",
    "\n",
    "intrinsic_variables = gw_parameters.intrinsic_variables\n",
    "extrinsic_variables = gw_parameters.extrinsic_variables\n",
    "\n",
    "trainer_settings = dict(\n",
    "    min_epochs = 1,\n",
    "    max_epochs = 10,\n",
    "    early_stopping = 7,\n",
    "    num_workers = 8,\n",
    "    training_batch_size = 256,\n",
    "    validation_batch_size = 256,\n",
    "    train_split = 0.9,\n",
    "    val_split = 0.1\n",
    ")\n",
    "\n",
    "network_settings = dict(\n",
    "    # Peregrine\n",
    "    shuffling = True,\n",
    "    include_noise = False,\n",
    "    priors = dict(\n",
    "        int_priors = conf['priors']['int_priors'],\n",
    "        ext_priors = conf['priors']['ext_priors'],\n",
    "    ),\n",
    "    marginals = ((0, 1),),\n",
    "    one_d_only = True,\n",
    "    ifo_list = conf[\"waveform_params\"][\"ifo_list\"],\n",
    "    learning_rate = 5e-4,\n",
    "    training_batch_size = trainer_settings['training_batch_size'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "simulation_store_path = '/scratch-shared/scur2012/training_data/default_limits_2e6/training_data'\n",
    "zarr_store = sl.ZarrStore(f\"{simulation_store_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dataloaders and setup trainer\n",
    "\n",
    "train_data = zarr_store.get_dataloader(\n",
    "    num_workers=trainer_settings['num_workers'],\n",
    "    batch_size=trainer_settings['training_batch_size'],\n",
    "    idx_range=[0, 30000],\n",
    "    on_after_load_sample=False\n",
    ")\n",
    "\n",
    "val_data = zarr_store.get_dataloader(\n",
    "    num_workers=trainer_settings['num_workers'],\n",
    "    batch_size=trainer_settings['validation_batch_size'],\n",
    "    idx_range=[30000, 35000],\n",
    "    on_after_load_sample=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/.conda/envs/peregrine/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Initialise network\n",
    "\n",
    "network = InferenceNetwork(**network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/.conda/envs/peregrine/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/scur2012/.conda/envs/peregrine/lib/python3.11/ ...\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "name_of_run = 'unet_attention_layer_1'\n",
    "\n",
    "# Make directory for logger\n",
    "logger_tbl = pl_loggers.TensorBoardLogger(\n",
    "    save_dir=f\"/home/scur2012/Thesis/master-thesis/experiments/unet_mods\",\n",
    "    name=f\"tb_logs\",\n",
    "    version=name_of_run,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "\n",
    "swyft_trainer = sl.SwyftTrainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    min_epochs=trainer_settings[\"min_epochs\"],\n",
    "    max_epochs=trainer_settings[\"max_epochs\"],\n",
    "    logger=logger_tbl,\n",
    "    enable_progress_bar = True,\n",
    "    val_check_interval = 50,\n",
    "    max_time = '00:00:10:00'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2012/.conda/envs/peregrine/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name         | Type                   | Params\n",
      "--------------------------------------------------------\n",
      "0 | netw_t       | Unet                   | 744 K \n",
      "1 | netw_f       | Unet                   | 744 K \n",
      "2 | flatten      | Flatten                | 0     \n",
      "3 | linear_t     | LinearCompression      | 0     \n",
      "4 | linear_f     | LinearCompression      | 0     \n",
      "5 | logratios_1d | LogRatioEstimator_1dim | 290 K \n",
      "--------------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.121     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b69955350d54c0fbfa48068c817d45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5227c8a8338c476d95e62278b60450aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e19a44fa99e449cb2cd458b19057f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a10fd2c7744149b07a3c6feb36f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2753191e08b24b8391c58a09d4d4acb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a826e0060e5434a912a3ca8f693d885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afed9156d8c4f268ff76aa7e3b5c017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d855fc545d4233939ccd91fc064757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28120d771540497ba1c892583708e776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d362297d478d43d8bf817bd3431082d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d644bf3d334615b2270b1f5ed640c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f270c265fed441dcb4471df66078c11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ccf23092ce4ecdab5c07320a0abe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea4c741b181477cb7dfdba852b75d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b21aa8bf63482d8e60207e0b228298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a65049c27f40fd93b91f1210937a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260daff5275045dd802bd30e3e69a8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e6d4e36998460bb79a2b7f003a00f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128fd82cc7e743fb88ed0e90be33c9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2374710042f04f24bb541985b87376c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6552ee25a27d4dc7a5b370358e1d5e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24f4dcb7fa94397ac30fcc038496204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "swyft_trainer.fit(network, train_data, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peregrine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
